{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac43bbc",
   "metadata": {},
   "source": [
    "# 03 – Modeling: Predicting High-Growth YouTube Videos\n",
    "\n",
    "This notebook trains and evaluates machine learning models to predict whether a\n",
    "YouTube video will become a **high-growth video** based on its early performance\n",
    "metrics and Google Trends signals.\n",
    "\n",
    "We use the final feature set produced in `02_feature_engineering.ipynb`, where\n",
    "YouTube metadata has been merged with category-level Google Trends scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc5465",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_modeling.ipynb\n",
    "# High-growth video prediction using YouTube and Google Trends features\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"Modeling notebook ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc15d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_modeling.ipynb\n",
    "# High-growth video prediction using YouTube and Google Trends features\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"Modeling notebook ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58798530",
   "metadata": {},
   "source": [
    "## 2. Load Features With Google Trends\n",
    "\n",
    "We load the processed feature set `features_with_trends.csv` from the\n",
    "`../data/processed/` directory. This dataset already contains:\n",
    "\n",
    "- YouTube video metrics (views, likes, comments, etc.)\n",
    "- Engineered ratios (like/view, comment/view)\n",
    "- Time-based features (publish hour, etc.)\n",
    "- Category-level Google Trends scores and rolling averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the final feature set (merged with Google Trends)\n",
    "features_path = \"../data/processed/features_with_trends.csv\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    features_path,\n",
    "    parse_dates=[\"trending_date\", \"publish_date\"],\n",
    ")\n",
    "\n",
    "print(\"Features shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b6f5f",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Missing Value Handling\n",
    "\n",
    "We define the list of features used for modeling. Note that we intentionally\n",
    "exclude future-dependent variables such as `view_growth` or `growth_rate` from\n",
    "the input features to avoid data leakage.\n",
    "\n",
    "Since Google Trends data is merged with a left join, some rows may not have a\n",
    "trend value. To ensure the models can be trained, we impute missing numeric\n",
    "values with the **median** of each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7eb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution of the target variable\n",
    "print(\"High-growth label distribution (fraction):\")\n",
    "print(df[\"high_growth\"].value_counts(normalize=True))\n",
    "\n",
    "# Features to be used in the models\n",
    "feature_cols = [\n",
    "    \"views\",\n",
    "    \"likes\",\n",
    "    \"dislikes\",\n",
    "    \"comment_count\",\n",
    "    \"like_view_ratio\",\n",
    "    \"comment_view_ratio\",\n",
    "    \"publish_hour\",\n",
    "    \"category_id\",\n",
    "    \"trend_score\",\n",
    "    \"trend_score_3d_mean\",\n",
    "    \"trend_score_7d_mean\",\n",
    "]\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nNumber of NaNs per feature before imputation:\")\n",
    "print(df[feature_cols].isna().sum())\n",
    "\n",
    "# Simple and robust imputation strategy:\n",
    "# - numeric features -> median\n",
    "# - non-numeric features (if any) -> -1 as a dummy category\n",
    "for col in feature_cols:\n",
    "    if df[col].dtype.kind in \"iuf\":  # int/unsigned/float\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    else:\n",
    "        df[col] = df[col].fillna(-1)\n",
    "\n",
    "print(\"\\nNumber of NaNs per feature after imputation:\")\n",
    "print(df[feature_cols].isna().sum())\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"high_growth\"].astype(int)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763c7aa",
   "metadata": {},
   "source": [
    "## 4. Time-Based Train / Validation / Test Split\n",
    "\n",
    "Instead of using a random split, we respect the temporal nature of the data:\n",
    "\n",
    "- The dataset is sorted by `trending_date`.\n",
    "- The oldest **60%** of the rows are used for **training**.\n",
    "- The next **20%** are reserved for **validation** (not explicitly tuned here,\n",
    "  but could be used for hyperparameter search).\n",
    "- The most recent **20%** are used for **testing**.\n",
    "\n",
    "This setup mimics a realistic scenario: we train models on past data and\n",
    "evaluate them on future videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based train/validation/test split\n",
    "df_sorted = df.sort_values(\"trending_date\").reset_index(drop=True)\n",
    "\n",
    "n = len(df_sorted)\n",
    "train_end = int(n * 0.6)\n",
    "val_end = int(n * 0.8)\n",
    "\n",
    "train = df_sorted.iloc[:train_end]\n",
    "val = df_sorted.iloc[train_end:val_end]\n",
    "test = df_sorted.iloc[val_end:]\n",
    "\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"high_growth\"].astype(int)\n",
    "\n",
    "X_val = val[feature_cols]\n",
    "y_val = val[\"high_growth\"].astype(int)\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "y_test = test[\"high_growth\"].astype(int)\n",
    "\n",
    "print(\"\\nLabel distribution (train, val, test):\")\n",
    "print(y_train.value_counts(normalize=True), \n",
    "      y_val.value_counts(normalize=True), \n",
    "      y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4e805",
   "metadata": {},
   "source": [
    "## 5. Models and Evaluation Metrics\n",
    "\n",
    "We evaluate two supervised classification models:\n",
    "\n",
    "1. **Logistic Regression** – a simple linear baseline.\n",
    "2. **Random Forest** – a non-linear ensemble model.\n",
    "\n",
    "For each model we report:\n",
    "\n",
    "- **Accuracy**\n",
    "- **F1-score**\n",
    "- **ROC-AUC**\n",
    "- A full classification report (precision, recall, F1 per class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, X_tr, y_tr, X_te, y_te, name=\"Model\"):\n",
    "    \"\"\"Prints basic evaluation metrics and returns them in a dictionary.\"\"\"\n",
    "    y_pred = model.predict(X_te)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_te)[:, 1]\n",
    "        roc = roc_auc_score(y_te, y_proba)\n",
    "    else:\n",
    "        y_scores = model.decision_function(X_te)\n",
    "        roc = roc_auc_score(y_te, y_scores)\n",
    "\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    f1 = f1_score(y_te, y_pred)\n",
    "\n",
    "    print(f\"\\n==== {name} ====\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1-score:\", f1)\n",
    "    print(\"ROC-AUC:\", roc)\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred))\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"roc_auc\": roc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: Logistic Regression\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "metrics_lr = evaluate_classifier(\n",
    "    log_reg, X_train, y_train, X_test, y_test, name=\"Logistic Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear model: Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "metrics_rf = evaluate_classifier(\n",
    "    rf, X_train, y_train, X_test, y_test, name=\"Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "metrics_xgb = evaluate_classifier(\n",
    "    xgb, X_train, y_train, X_test, y_test, name=\"XGBoost\"\n",
    ")\n",
    "\n",
    "metrics_xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb22bd8",
   "metadata": {},
   "source": [
    "## 6. Random Forest Confusion Matrix and Feature Importance\n",
    "\n",
    "Random Forest often performs better when there are non-linear relationships\n",
    "and interactions between features.\n",
    "\n",
    "We inspect:\n",
    "\n",
    "- The confusion matrix on the test set.\n",
    "- Feature importances estimated by the Random Forest model.\n",
    "\n",
    "This helps us understand which signals (e.g., early views, engagement ratios,\n",
    "Google Trends scores) contribute the most to predicting high-growth videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Random Forest\n",
    "cm = confusion_matrix(y_test, rf.predict(X_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_title(\"Confusion Matrix - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "importances = rf.feature_importances_\n",
    "fi = pd.Series(importances, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "fi.plot(kind=\"bar\")\n",
    "plt.title(\"Feature Importances (Random Forest)\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b675c",
   "metadata": {},
   "source": [
    "## 7. Impact of Google Trends Features\n",
    "\n",
    "To quantify the added value of Google Trends, we compare two Random Forest models:\n",
    "\n",
    "- **RF WITHOUT Trends**: trained on YouTube features only  \n",
    "  (`views`, `likes`, `comment_view_ratio`, `publish_hour`, `category_id`, etc.).\n",
    "\n",
    "- **RF WITH Trends**: trained on the full feature set, including  \n",
    "  `trend_score`, `trend_score_3d_mean`, `trend_score_7d_mean`.\n",
    "\n",
    "By comparing their Accuracy, F1-score, and ROC-AUC on the same test set, we can\n",
    "assess whether Google Trends provides additional predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14787070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend-related columns\n",
    "trend_cols = [\"trend_score\", \"trend_score_3d_mean\", \"trend_score_7d_mean\"]\n",
    "\n",
    "base_features = [c for c in feature_cols if c not in trend_cols]\n",
    "trend_features = feature_cols  # all features\n",
    "\n",
    "\n",
    "def train_eval_rf(feature_list, name=\"\"):\n",
    "    X_train_f = train[feature_list]\n",
    "    X_test_f = test[feature_list]\n",
    "    y_train_f = train[\"high_growth\"].astype(int)\n",
    "    y_test_f = test[\"high_growth\"].astype(int)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X_train_f, y_train_f)\n",
    "\n",
    "    y_pred = model.predict(X_test_f)\n",
    "    y_proba = model.predict_proba(X_test_f)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test_f, y_pred)\n",
    "    f1 = f1_score(y_test_f, y_pred)\n",
    "    roc = roc_auc_score(y_test_f, y_proba)\n",
    "\n",
    "    print(f\"\\n==== {name} ====\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"ROC-AUC:\", roc)\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"roc_auc\": roc}\n",
    "\n",
    "\n",
    "metrics_rf_no_trend = train_eval_rf(base_features, name=\"RF WITHOUT Trends\")\n",
    "metrics_rf_with_trend = train_eval_rf(trend_features, name=\"RF WITH Trends\")\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"RF_without_trends\", \"RF_with_trends\"],\n",
    "        \"accuracy\": [metrics_rf_no_trend[\"accuracy\"], metrics_rf_with_trend[\"accuracy\"]],\n",
    "        \"f1\": [metrics_rf_no_trend[\"f1\"], metrics_rf_with_trend[\"f1\"]],\n",
    "        \"roc_auc\": [metrics_rf_no_trend[\"roc_auc\"], metrics_rf_with_trend[\"roc_auc\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40dad3",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "- The Random Forest model typically outperforms Logistic Regression on the test\n",
    "  set, suggesting non-linear relationships between features and the `high_growth`\n",
    "  label.\n",
    "- Adding Google Trends features often leads to an improvement in overall\n",
    "  performance (especially ROC-AUC and/or F1-score), indicating that external\n",
    "  trend signals carry useful information beyond YouTube-internal metrics.\n",
    "- The most important features usually include early views, engagement ratios\n",
    "  (like/view, comment/view), and category-level trend scores.\n",
    "- The final model can be exported locally as a `.pkl` file for deployment or\n",
    "  further analysis.\n",
    "\n",
    "> **Note:** Model artifacts (`.pkl` files) are intentionally not stored in the\n",
    "> GitHub repository to avoid large binary files. They can be regenerated by\n",
    "> rerunning this notebook, or stored in external storage if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719dc1e",
   "metadata": {},
   "source": [
    "## 9. (Optional) Save the Trained Model Locally\n",
    "\n",
    "If you want to save the trained Random Forest model for later use (outside of\n",
    "this notebook), you can uncomment and run the following cell. This will create\n",
    "a `models/` directory (if it does not exist) and store the model as a `.pkl`\n",
    "file **on your local machine only**.\n",
    "\n",
    "Remember **not to commit or push** such large model files to GitHub; instead,\n",
    "add `models/` and `*.pkl` to your `.gitignore`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# import os\n",
    "\n",
    "# os.makedirs(\"../models\", exist_ok=True)\n",
    "# model_path = \"../models/final_rf_model.pkl\"\n",
    "# joblib.dump(rf, model_path)\n",
    "# print(f\"Random Forest model saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
