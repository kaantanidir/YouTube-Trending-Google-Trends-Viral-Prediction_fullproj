{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8256065c",
   "metadata": {},
   "source": [
    "# 02 – Feature Engineering for US YouTube Trending Videos\n",
    "\n",
    "This notebook performs feature engineering for the US YouTube trending videos\n",
    "dataset and prepares two processed datasets:\n",
    "\n",
    "1. `../data/processed/features.csv` – base features for each video-day.\n",
    "2. `../data/processed/features_with_trends.csv` – the same feature set enriched\n",
    "   with category-level Google Trends scores.\n",
    "\n",
    "The output of this notebook is used later in `03_modeling.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166341e",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ec0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02_feature_engineering.ipynb\n",
    "# Core feature engineering steps for the US YouTube Trending dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Feature engineering notebook ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf42c1",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data and Basic Ratios\n",
    "\n",
    "We start from the raw US YouTube trending dataset stored at\n",
    "`../data/raw/USvideos.csv`. For convenience, we also recompute the basic\n",
    "engagement ratios so that this notebook can be run standalone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw US YouTube trending data\n",
    "df = pd.read_csv(\"../data/raw/USvideos.csv\")\n",
    "\n",
    "# Recreate ratios in case this notebook is run standalone\n",
    "df[\"like_view_ratio\"] = df[\"likes\"] / (df[\"views\"] + 1e-6)\n",
    "df[\"comment_view_ratio\"] = df[\"comment_count\"] / (df[\"views\"] + 1e-6)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd572fd",
   "metadata": {},
   "source": [
    "## 3. Publish Time Parsing\n",
    "\n",
    "The `publish_time` column is parsed into:\n",
    "\n",
    "- `publish_time` (datetime)\n",
    "- `publish_date` (date only)\n",
    "- `publish_hour` (hour of day)\n",
    "\n",
    "These are useful temporal features for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse publish_time into datetime, date and hour components\n",
    "df[\"publish_time\"] = pd.to_datetime(df[\"publish_time\"], errors=\"coerce\")\n",
    "df[\"publish_date\"] = df[\"publish_time\"].dt.date\n",
    "df[\"publish_hour\"] = df[\"publish_time\"].dt.hour\n",
    "\n",
    "df[[\"publish_time\", \"publish_date\", \"publish_hour\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5449abd",
   "metadata": {},
   "source": [
    "## 4. Fix `trending_date` Format\n",
    "\n",
    "In the original Kaggle dataset, the `trending_date` column is stored as\n",
    "`YY.DD.MM`, for example `'17.14.11'` meaning **2017‑11‑14**.\n",
    "\n",
    "We convert it into a proper `datetime` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d31a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix trending_date format: original is 'YY.DD.MM', e.g. '17.14.11' meaning 2017-11-14\n",
    "def fix_trending_date(x: str) -> str:\n",
    "    yy, dd, mm = x.split(\".\")\n",
    "    return f\"20{yy}-{mm}-{dd}\"\n",
    "\n",
    "df[\"trending_date_fixed\"] = df[\"trending_date\"].apply(fix_trending_date)\n",
    "df[\"trending_date\"] = pd.to_datetime(df[\"trending_date_fixed\"], errors=\"coerce\")\n",
    "df = df.drop(columns=[\"trending_date_fixed\"])\n",
    "\n",
    "df[[\"trending_date\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc8a4a",
   "metadata": {},
   "source": [
    "## 5. Next-Day Views and Growth Metrics\n",
    "\n",
    "To understand how fast a video is growing, we compare the current day's view\n",
    "count with the **next trending day's** view count for the same video.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Sort by `video_id` and `trending_date`.\n",
    "2. For each `video_id`, shift the `views` column by -1 to get `views_next_day`.\n",
    "3. Compute absolute and relative growth:\n",
    "   - `view_growth = views_next_day - views`\n",
    "   - `growth_rate = view_growth / views`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacf040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by video_id and trending_date so that we can compute next-day views\n",
    "df = df.sort_values(by=[\"video_id\", \"trending_date\"])\n",
    "df[[\"video_id\", \"trending_date\", \"views\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute next-day view count per video\n",
    "df[\"views_next_day\"] = df.groupby(\"video_id\")[\"views\"].shift(-1)\n",
    "\n",
    "df[[\"video_id\", \"trending_date\", \"views\", \"views_next_day\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute and relative growth\n",
    "df[\"view_growth\"] = df[\"views_next_day\"] - df[\"views\"]\n",
    "df[\"growth_rate\"] = df[\"view_growth\"] / (df[\"views\"] + 1e-6)\n",
    "\n",
    "df[[\"views\", \"views_next_day\", \"view_growth\", \"growth_rate\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc4d49",
   "metadata": {},
   "source": [
    "## 6. Define the High-Growth Label\n",
    "\n",
    "We restrict ourselves to rows where `growth_rate` is defined (i.e. not the last\n",
    "trending day of each video), and define a **binary label**:\n",
    "\n",
    "- `high_growth = 1` if `growth_rate` is in the top 25% (≥ 75th percentile).\n",
    "- `high_growth = 0` otherwise.\n",
    "\n",
    "This label is what we predict later in `03_modeling.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where growth_rate is NaN (typically the last trending day of each video)\n",
    "df_valid = df.dropna(subset=[\"growth_rate\"]).copy()\n",
    "\n",
    "# Define high_growth label as top 25% of growth_rate\n",
    "threshold = df_valid[\"growth_rate\"].quantile(0.75)\n",
    "df_valid[\"high_growth\"] = (df_valid[\"growth_rate\"] >= threshold).astype(int)\n",
    "\n",
    "print(\"High growth threshold (75th percentile):\", threshold)\n",
    "df_valid[\"high_growth\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19787b",
   "metadata": {},
   "source": [
    "## 7. Select Feature Columns and Save `features.csv`\n",
    "\n",
    "We keep the following columns as our *base* feature set for modeling and save\n",
    "them to `../data/processed/features.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feature columns for modeling\n",
    "feature_cols = [\n",
    "    \"video_id\",\n",
    "    \"trending_date\",\n",
    "    \"publish_date\",\n",
    "    \"publish_hour\",\n",
    "    \"views\",\n",
    "    \"likes\",\n",
    "    \"dislikes\",\n",
    "    \"comment_count\",\n",
    "    \"like_view_ratio\",\n",
    "    \"comment_view_ratio\",\n",
    "    \"view_growth\",\n",
    "    \"growth_rate\",\n",
    "    \"high_growth\",\n",
    "    \"category_id\",\n",
    "]\n",
    "\n",
    "features = df_valid[feature_cols].copy()\n",
    "print(\"Processed features shape:\", features.shape)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaae7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed features\n",
    "features.to_csv(\"../data/processed/features.csv\", index=False)\n",
    "print(\"Saved processed features to ../data/processed/features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db6802",
   "metadata": {},
   "source": [
    "## 8. Enrich Features with Google Trends\n",
    "\n",
    "Finally, we merge the base `features.csv` with the category-level Google Trends\n",
    "data stored in `../data/raw/google_trends_category.csv`.\n",
    "\n",
    "The merge is done on:\n",
    "\n",
    "- `trending_date` (from the features table)\n",
    "- `category_id` (YouTube category)\n",
    "\n",
    "We also compute 3-day and 7-day rolling averages of the trend score per\n",
    "category to smooth out noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base features table (one row per video-day)\n",
    "df_feat = pd.read_csv(\n",
    "    \"../data/processed/features.csv\",\n",
    "    parse_dates=[\"trending_date\", \"publish_date\"],\n",
    ")\n",
    "\n",
    "print(\"Base features shape:\", df_feat.shape)\n",
    "\n",
    "# Load Google Trends data\n",
    "trends = pd.read_csv(\n",
    "    \"../data/raw/google_trends_category.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "\n",
    "print(\"Google Trends shape:\", trends.shape)\n",
    "\n",
    "# Join on date and category_id\n",
    "merged = df_feat.merge(\n",
    "    trends,\n",
    "    left_on=[\"trending_date\", \"category_id\"],\n",
    "    right_on=[\"date\", \"category_id\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Drop helper columns not needed anymore\n",
    "merged = merged.drop(columns=[\"date\", \"keyword\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Merged shape (features + trends):\", merged.shape)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rolling mean trend scores per category\n",
    "merged = merged.sort_values([\"category_id\", \"trending_date\"])\n",
    "\n",
    "merged[\"trend_score_3d_mean\"] = (\n",
    "    merged.groupby(\"category_id\")[\"trend_score\"]\n",
    "    .transform(lambda s: s.rolling(window=3, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "merged[\"trend_score_7d_mean\"] = (\n",
    "    merged.groupby(\"category_id\")[\"trend_score\"]\n",
    "    .transform(lambda s: s.rolling(window=7, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# Save final feature set enriched with trends\n",
    "merged.to_csv(\"../data/processed/features_with_trends.csv\", index=False)\n",
    "print(\"Saved features_with_trends to ../data/processed/features_with_trends.csv\")\n",
    "\n",
    "merged.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
